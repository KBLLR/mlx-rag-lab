# Session Log Template

Use this template to document each working session **before committing**. Duplicate the structure below, fill in every field, and save the entry alongside your work (e.g., `docs/sessions/YYYY-MM-DD-session.md`). Keep language concise and candid—this log is meant to surface decisions, errors, and insights for future contributors.

---

## Session Summary
- **Date**: `2025-11-07`
- **Start Time**: `13:00` (local time)
- **End Time**: `14:00`
- **Elapsed (HH:MM)**: `01:00`
- **Working Title**: `Implement Re-ranking and Source Citations`
- **Associated Tasks / Issues**: `RAG-013`

## Objectives
- Primary goal(s) for this session:
  - Address incorrect source citations from the LLM.
  - Improve retrieval quality for ambiguous questions.
  - Implement a cross-encoder re-ranking pipeline.
  - Enable accurate source tracking for each retrieved chunk.

## Execution Notes
- Entry point (files, scripts, commands):
  - Modified `src/rag/retrieval/vdb.py` to change the internal content structure.
  - Modified `src/rag/cli/interactive_rag.py` to adapt to the new VDB structure and implement the re-ranking workflow.
  - Created `src/rag/models/cross_encoder.py` with a placeholder implementation.
  - Removed `src/rag/models/cross_encoder.py`.
  - Created `src/rag/models/qwen_reranker.py` with a `mlx_lm` based Qwen2 reranker implementation.
  - Created `src/rag/mlx_compat.py` for `mlx_lm.generate_step` compatibility.
  - Added `rich.progress` to `src/rag/ingestion/create_vdb.py` for visual progress during PDF processing.
- Key changes made:
  - `VectorDB` now stores a dictionary `{"text": chunk, "source": doc_name}` for each item in its `content` list.
  - The `ingest`, `query`, and `savez` methods in `VectorDB` were updated to handle this new data structure.
  - The `rebuild_vdb` command in the CLI now correctly passes the `document_name` during ingestion.
  - The `ask_question` command now retrieves a larger set of candidates (k=20), uses a `QwenReranker` to re-rank them, and passes the top 5 to the LLM.
  - The prompt `TEMPLATE` was updated to instruct the LLM on how to use the new, richer context with source information.
  - The `list-docs` command was updated to derive the unique document list from the new content structure.
  - The `CrossEncoder` class was replaced with `QwenReranker` which uses `mlx_lm.load` to load the `mlx-community/mxbai-rerank-large-v2` model.
  - `src/rag/cli/interactive_rag.py` was updated to import and instantiate `QwenReranker`.
  - Fixed `NameError: name 'dataclass' is not defined` by adding `from dataclasses import dataclass` in `src/rag/models/qwen_reranker.py`.
  - Fixed `NameError: name 'mx' is not defined` by adding `import mlx.core as mx` in `src/rag/models/qwen_reranker.py`.
  - Addressed `TypeError: generate_step() got an unexpected keyword argument 'temperature'` by implementing a compatibility wrapper `src/rag/mlx_compat.py` and updating `src/rag/models/qwen_reranker.py` to use it.
- Tests / commands executed:
  - `uv run python -m rag.cli.interactive_rag`
  - `uv run python -m cProfile -o profile_output.prof src/rag/cli/interactive_rag.py rebuild-vdb`
  - `uv run snakeviz profile_output.prof`
  - `rebuild-vdb`
  - `list-docs`
  - `ask`

## Reflection
- **Errors Encountered**:
  - Encountered and fixed a `NameError` for `Dict` in `vdb.py`.
  - Encountered and fixed a GPU `Insufficient Memory` error during `rebuild-vdb` by implementing incremental, per-document ingestion.
  - Encountered and fixed several `TypeError` and API mismatch errors related to `mlx-lm`'s `stream_generate` function and `GenerationResponse` object.
  - Encountered `TypeError: MlxPretrainedMixin.from_pretrained() missing 1 required positional argument: 'model_name_or_path'` when attempting to load `mlx-community/mxbai-rerank-large-v2` with `mlx_transformers.models.BertForSequenceClassification.from_pretrained`.
  - **Resolution**: Discovered that `mlx-community/mxbai-rerank-large-v2` is a Qwen2 causal LM, not a BERT-style cross-encoder, and should be loaded with `mlx_lm.load`. Refactored the reranker implementation to use `mlx_lm`.
  - Encountered `ImportError: cannot import name 'load' from 'mlx_transformers'` and `ModuleNotFoundError: No module named 'mlx_transformers.utils'` due to API changes in `mlx_transformers`.
  - Encountered `TypeError: generate_step() got an unexpected keyword argument 'temperature'` due to API changes in `mlx_lm`.
  - Encountered `NameError: name 'dataclass' is not defined` and `NameError: name 'mx' is not defined` due to missing imports.
  - Encountered `sh: command not found: snakeviz` when trying to visualize `cProfile` output.
  - **Resolution**: Instructed to use `uv run snakeviz profile_output.prof` to execute `snakeviz` within the virtual environment.
- **Decisions Taken**:
  - Decided to implement a native MLX cross-encoder placeholder rather than add the `sentence-transformers` (and PyTorch) dependency, aligning with the project's philosophy.
  - Switched from `mlx_transformers.models.BertForSequenceClassification` to a custom `QwenReranker` class utilizing `mlx_lm.load` for the `mlx-community/mxbai-rerank-large-v2` model, as it is a Qwen2 architecture.
  - Implemented a compatibility wrapper for `mlx_lm.generate_step` to handle API changes.
  - Integrated `rich.progress` for better visibility during PDF ingestion.
- **Learnings & Surprises**:
  - The MLX ecosystem is still evolving, and native, pre-trained models for tasks like cross-encoding are not as readily available as in the broader PyTorch/Hugging Face ecosystem. This necessitates a more hands-on approach.
  - Processing large amounts of text for embedding in a single batch is a common cause of GPU memory errors.
  - It is crucial to verify the actual architecture and intended loading mechanism of MLX-converted models on Hugging Face Hub, as `mlx-transformers` and `mlx-lm` cater to different model families.
  - Python virtual environment executables need to be run via `uv run` or have their `bin` directory added to `PATH`.

## Next Actions
- Immediate follow-ups:
  - `RAG-013` is now complete.
  - The monitoring setup is in place for future performance analysis.
- Blockers / dependencies:
  - None.

## Session Quote
- *Famous quote that captured the tone of the session*:  
  > “The details are not the details. They make the design.” — *Charles Eames*

## Post Image Prompt
- Use the following prompt to generate an illustrative image for the post/session recap:  
  ```
  A data pipeline showing documents being retrieved, then re-ordered by a ranking engine, and finally fed into a large language model, in a clean, technical style with glowing lines.
  ```

---

> **Reminder:** Attach this session log (or link to it) in the PR description so reviewers can quickly understand context, decisions, and outstanding work.