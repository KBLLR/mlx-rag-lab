"""
Phi-3-Vision-MLX Backend
========================

Thin wrapper around the phi-3-vision-mlx library for standardized model loading and generation.
This module provides a clean interface for the RAG system to interact with Phi-3-Vision models.

Author: Generated by Claude Code
Date: 2025-11-13
"""

import sys
from pathlib import Path
from typing import List, Optional, Tuple, Union

# Add Phi-3-Vision-MLX to path
PHI3_VISION_PATH = Path(__file__).resolve().parents[3] / "mlx-models" / "Phi-3-Vision-MLX"
if str(PHI3_VISION_PATH) not in sys.path:
    sys.path.insert(0, str(PHI3_VISION_PATH))

try:
    # Import the core module directly to avoid gradio dependencies
    from phi_3_vision_mlx.phi_3_vision_mlx import (
        generate as _generate_raw,
        Agent as _Agent_raw,
    )
    # We'll create wrappers for the functions we need
    pv = None  # Placeholder for compatibility
    VISION_AVAILABLE = True
except ImportError as e:
    _generate_raw = None
    _Agent_raw = None
    VISION_AVAILABLE = False
    _VISION_ERROR = str(e)

def _check_vision_available():
    """Check if vision backend is available."""
    if not VISION_AVAILABLE:
        raise ImportError(
            f"Phi-3-Vision backend is not available. "
            f"Original error: {_VISION_ERROR}\n\n"
            f"To use vision capabilities, install the required dependencies:\n"
            f"  uv add gradio starlette datasets\n\n"
            f"Or install from requirements:\n"
            f"  uv pip install -r {PHI3_VISION_PATH}/requirements.txt"
        )


class Phi3VisionBackend:
    """
    Backend wrapper for Phi-3-Vision-MLX model.

    Provides a standardized interface for loading and using the Phi-3-Vision model
    with various configuration options.

    Attributes:
        blind_model (bool): Whether to use language-only model (no vision).
        quantize_model (bool): Whether to use quantized model.
        quantize_cache (bool): Whether to quantize KV cache.
        use_adapter (bool): Whether to use LoRA adapter.
        preloaded (tuple): Preloaded (model, processor) tuple.
    """

    def __init__(
        self,
        blind_model: bool = False,
        quantize_model: bool = False,
        quantize_cache: bool = False,
        use_adapter: bool = False,
        preload: bool = True,
    ):
        """
        Initialize the Phi-3-Vision backend.

        Args:
            blind_model: Use language-only model (no vision capabilities).
            quantize_model: Use quantized version of the model.
            quantize_cache: Use quantized KV cache for memory efficiency.
            use_adapter: Use LoRA adapter if available.
            preload: Preload the model on initialization.
        """
        _check_vision_available()

        self.blind_model = blind_model
        self.quantize_model = quantize_model
        self.quantize_cache = quantize_cache
        self.use_adapter = use_adapter
        self.preloaded = None

        if preload:
            self.load()

    def load(self) -> Tuple:
        """
        Load the model and processor.

        Returns:
            Tuple of (model, processor).
        """
        # Delegate to phi_3_vision_mlx.load()
        # Note: We need to check if load() is exposed in the API
        # For now, we'll let generate() handle the loading
        self.preloaded = None  # Will be loaded on first generate() call
        return self.preloaded

    def generate(
        self,
        prompt: Union[str, List[str]],
        images: Optional[Union[str, List[str]]] = None,
        max_tokens: int = 512,
        verbose: bool = False,
        stream: bool = False,
        apply_chat_template: bool = True,
        early_stop: Union[bool, int] = False,
    ) -> Union[str, List[str]]:
        """
        Generate text based on prompt and optional images.

        Args:
            prompt: Text prompt(s) for generation.
            images: Optional image path(s) or URL(s).
            max_tokens: Maximum tokens to generate.
            verbose: Print generation info.
            stream: Stream output as it's generated.
            apply_chat_template: Apply chat template to prompt.
            early_stop: Stop early under certain conditions.

        Returns:
            Generated text (str or list of str).
        """
        result = _generate_raw(
            prompt=prompt,
            images=images,
            preload=self.preloaded,
            blind_model=self.blind_model,
            quantize_model=self.quantize_model,
            quantize_cache=self.quantize_cache,
            use_adapter=self.use_adapter,
            max_tokens=max_tokens,
            verbose=verbose,
            stream=stream,
            apply_chat_template=apply_chat_template,
            early_stop=early_stop,
            return_tps=False,
        )
        return result

    def create_agent(self, toolchain: Optional[str] = None, **kwargs):
        """
        Create an Agent instance for multi-turn conversations.

        Args:
            toolchain: Custom toolchain string.
            **kwargs: Additional kwargs for Agent.

        Returns:
            Agent instance.
        """
        return _Agent_raw(toolchain=toolchain, **kwargs)


def load_phi3_vision(
    blind_model: bool = False,
    quantize_model: bool = False,
    quantize_cache: bool = False,
    use_adapter: bool = False,
) -> Phi3VisionBackend:
    """
    Factory function to create and load a Phi-3-Vision backend.

    Args:
        blind_model: Use language-only model.
        quantize_model: Use quantized model.
        quantize_cache: Use quantized cache.
        use_adapter: Use LoRA adapter.

    Returns:
        Configured Phi3VisionBackend instance.
    """
    return Phi3VisionBackend(
        blind_model=blind_model,
        quantize_model=quantize_model,
        quantize_cache=quantize_cache,
        use_adapter=use_adapter,
        preload=True,
    )


# Convenience functions for common use cases

def simple_vqa(prompt: str, image_path: str, quantize: bool = True) -> str:
    """
    Simple visual question answering.

    Args:
        prompt: Question about the image.
        image_path: Path or URL to image.
        quantize: Use quantized model for efficiency.

    Returns:
        Answer as string.
    """
    backend = Phi3VisionBackend(quantize_model=quantize, preload=False)
    return backend.generate(prompt, images=[image_path], verbose=False)


def batch_vqa(
    prompts: List[str],
    images: List[str],
    quantize: bool = True,
) -> List[str]:
    """
    Batch visual question answering.

    Args:
        prompts: List of questions.
        images: List of image paths/URLs.
        quantize: Use quantized model.

    Returns:
        List of answers.
    """
    backend = Phi3VisionBackend(quantize_model=quantize, preload=False)
    return backend.generate(prompts, images=images, verbose=False)


def is_vision_available() -> bool:
    """
    Check if vision backend is available without raising an error.

    Returns:
        True if vision backend can be used, False otherwise.
    """
    return VISION_AVAILABLE


if __name__ == "__main__":
    # Test the backend
    print("Testing Phi-3-Vision Backend...")
    print(f"Vision available: {is_vision_available()}")

    if not is_vision_available():
        print("\nVision backend is not available. Install dependencies:")
        print(f"  uv add gradio starlette datasets")
        print("\nOr:")
        print(f"  uv pip install -r {PHI3_VISION_PATH}/requirements.txt")
    else:
        try:
            # Test language-only generation
            backend = Phi3VisionBackend(blind_model=True, quantize_model=True)
            result = backend.generate("Explain MLX in one sentence.", max_tokens=50)
            print(f"\nLanguage-only test: {result}")

            # Test VQA (would need an actual image)
            # backend_vision = Phi3VisionBackend(quantize_model=True)
            # result_vqa = backend_vision.generate(
            #     "What's in this image?",
            #     images=["path/to/image.jpg"]
            # )
            # print(f"VQA test: {result_vqa}")

            print("\nBackend test completed successfully!")
        except Exception as e:
            print(f"\nError during backend test: {e}")
            import traceback
            traceback.print_exc()
